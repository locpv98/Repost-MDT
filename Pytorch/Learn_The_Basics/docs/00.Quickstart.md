# QUICKSTAR
This section runs through the API for common tasks in machine learning. Refer to the link in each section to dive deeper.
## Working with data
PyTorch has two primitives (nguyen thuy) to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.DataSet`. `Dataset` stores the samples
and their corresponding (tuong ung) labels, and `DataLoader` wraps an iterable (co the lap lai) around the `Dataset`.

```python
import torch  
from torch import nn  
from torch.utils.data import DataLoader  
from torchvision import datasets  
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt
```
PyTorch offers (cung cap) domain-specific (mien cu the) libraries such as **TorchText, TorchVision** and **TorchAudio**, all of which include datasets. For this tutorial, we will be using  
a TorchVision dataset.  
The `torchvision.dataset` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial we use the FashionMNIST dataset.  
Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.
```python
# Download training data from open datasets.
training_data = datasets.FashionMNIST(
  root = 'data',
  train = True,
  dowload = True,
  transform = ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
  root = 'data',
  train = True,
  dowload = True,
  transform = ToTensor(),
)
```
We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching (lo hang), sampling (lay mau),  
shuffling (xoa tron) and multiprocess data loading. Here we define a batch size of 64, i.c.each element in the dataloader iterable will return a batch of 64 features and labels.
```python
batch_size = 64

# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size = batch_size)
test_dataloader = DataLoader(test_data, batch_size = batch_size)

for X, y in test_dataloader:
  print("Shape of X [N, C, H, W]:", X.shape)
  print("Shape of y: ", y.shape, y.dtype)
  break
```
Out:
```
Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64
```
We can read more about loanding data in PyTorch
## Creating Models
The define a neural network in PyTorch, we create a class that inherits (kethua) from **nn.Module**. We define the layers of the network in the __init__
function and spectify how data will pass throught the network in the `forward` function. To accelerate (tang toc do) operations (hoat dong) in the neural network, we move it to GPU if available.
```python
# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
            nn.ReLU()
        )
    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
model = NeuralNetwork().to(device)
print(model)
```
## Optimizing the Model Parameters
To train a model, we need a lossfuntion and on optimizer.  
In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the preidiction error to adjust the  
model's parameters.  
We also check the model's performance against against the test datatet to ensure it is learning.  
The training process is conducted over several iterations (*epoch*). During each epochm the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch: we'd like to see the accuracy increase and the loss decrease with every epoch.
## Saving Models
A common way to save a model is to serialize the internal state dictionary (containing the model parameters).  
Out:
## Loading Models
The process for loading a model includes re-creating the mode structure and loading the state dictionary into it.  
This model can now be used to make predictions.
