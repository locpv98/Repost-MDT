# QUICKSTAR
This section runs through the API for common tasks in machine learning. Refer to the link in each section to dive deeper.
## Working with data
PyTorch has two primitives (nguyen thuy) to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.DataSet`. `Dataset` stores the samples
and their corresponding (tuong ung) labels, and `DataLoader` wraps an iterable (co the lap lai) around the `Dataset`.

```python
import torch  
from torch import nn  
from torch.utils.data import DataLoader  
from torchvision import datasets  
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt
```
PyTorch offers (cung cap) domain-specific (mien cu the) libraries such as **TorchText, TorchVision** and **TorchAudio**, all of which include datasets. For this tutorial, we will be using  
a TorchVision dataset.  
The `torchvision.dataset` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial we use the FashionMNIST dataset.  
Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.
```python
# Download training data from open datasets.
training_data = datasets.FashionMNIST(
  root = 'data',
  train = True,
  dowload = True,
  transform = ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
  root = 'data',
  train = True,
  dowload = True,
  transform = ToTensor(),
)
```
We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching (lo hang), sampling (lay mau),  
shuffling (xoa tron) and multiprocess data loading.
## Creating Models
The define a neural network in PyTorch, we create a class that inherits from **nn.Module**. We define the layers of the network in the __init__
function and spectfy how data will pass throught the network in the `forward` function. To accelerate operations in the neural network, we move it to GPU if available.
## Optimizing the Model Parameters
To train a model, we need a lossfuntion and on optimizer.  
In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the preidiction error to adjust the  
model's parameters.  
We also check the model's performance against against the test datatet to ensure it is learning.  
The training process is conducted over several iterations (*epoch*). During each epochm the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch: we'd like to see the accuracy increase and the loss decrease with every epoch.
## Saving Models
A common way to save a model is to serialize the internal state dictionary (containing the model parameters).  
Out:
## Loading Models
The process for loading a model includes re-creating the mode structure and loading the state dictionary into it.  
This model can now be used to make predictions.
