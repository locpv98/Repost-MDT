# Building a Face Mask Recognition System With TensorFlow
> Everything is automated nowadays. Instead of having manual screenings, why not have a system that automatically detects face mask usage?

In the era of Covid-19, masking up and social distancing have become the new norm. Indoor places, such as restaurants and grocery stores, are legally required to put in palace rules for mandatory face mask usage. Having a worker manually screen every person to ensure their mask is on just defeats the purpose of limiting contact with people as much as possible.
The sophistication of TENSORFLOW and the OpenCV library have made it possible to create an automated solution to address this issue that will not only maximize efficiency and ensure compliance, but also potentially save lives. Businesses could implement this system to enforce face mask requirements in place and automatically refuse entry to people who are not wearing a mask, avoiding face-to-face contact that oftentimes lead to unnecessary conflict. In fact, this practice is already in place in VIETNAM, and it’s no wonder that their country has been hailed as a Covid-19 response success model. Coincidence? Maybe yes, but let’s just say that this system played a part in that.
We see computer vision applications of image recognition technology in our everyday lives all too often. Whether it is unlocking your iPhone through facial recognition, going through airport screenings, or even passing by toll gantries that capture images of your car as it passes through, image classification exists to have machines efficiently achieve what we program them to. Whether it is any of the aforementioned of a face mask recognition system, technology should be incorporated in our daily lives for social good. Our goal is to do just that - To create a face mask recognition system that is intelligible for everyone to understand how image classification works, and so that our project could be applied and replicated in practice in real life settings. For the curious-minded, here’s how we created our Face Mask Recognition system with TensorFlow that detects your facial boundaries and predicts whether or not you are wearing a face mask in real time. Key snippets of code will be show-click here to access the full code and dataset in Github.

## DATA COLLECTION
First, we need collection images for our training and test data sets. We would like to create our own dataset consisting of images of people with a face mask on, and images of people without a face mask. We leveraged the Selenium and BeautifulSoup libraries in Python to automate the web browser to parse royalty-free images on Shutterstock.com. We created a script that asks for user input on what type of image the user would like to scrape (photos or any type of image) and what their search query is for that image. Furthermore, the user may specify how many pages to scrape from.

## DATA PREPROCESSING
After scraping 606 images of people with a face mask on and 665 images of people without a face mask, we created a training set and test set that contains these images with the following specifications: 80% of randomly selected images of people with face mask go to training set, 20% of randomly selected images of people with face mask go to test set. This process is repeated for training and test data sets for images of people without a face mask.
With only a few images in our disposal, we can still build a powerful image classifier using ImageDataGenerator in Keras to generate batches of tensor (multidimensional array) image data with real-time data augmentation to increase the amount and diversity of the dataset. The data augmentation process works by creating duplicate pictures such as adding a filter and flipping the original image, which was done in order to increase our data size without having to collect new images while also helping train the model to capture differences in positioning, image quality, and appearance.

## BUILDING THE MODEL
In machine learning, we often find the best solutions to real world problems by modeling a framework after the natural world. With image classification, we are essentially (bản chất) training computers to exhibit (chưng bày) the functionality (chức năng) of the human eye by training it through a series of algorithms in artificial neural networks that are modeled after the biological neural network. For this project, we designed our model with the classic building blocks to build a CNN model due to its widespread popularity in image recognition for its generation of high accuracy rates. CNN follows a hierarchical (thu bac) model which works on building a funnel-like network that outputs a fully-connected layer where all neurons are connected to each other and a classification probability is determined.
Every image is considered (dc xem nhu) a matrix of pixel values. Each matrix cell contains 3 channels (RED<, green and blue) which give the color saturation (bao hoa). On every image of our dataset, the convolutional layer applies 100 different filters, or kernels, of size 3x3, which means the filter is striding by 1 pixel as depicted in the following animation (hoat hinh dong).
The image on the left provides a great visual to how each feature map is derived (nguon goc). Because we used 100 filters for our convolutional layer, we will thus output 100 feature maps-one for each image in our dataset.
So why 100 filters?. Because the higher the number of filters we use, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images.
Finally, we decided to use three Dense layers (fully-connected layers) in our model with 50, 35 and ultimately (cuoi cung) 2 neurons, respectively (tuong ung). The dense (day dac) network output the probability from binary classification of no mask = 1 and mask = 0.
Combined with the Adam optimizer, an extension to SGD that uses momentum and adaptive learning retas to converge faster, and binary cross-entropy as our loss function that outputs the mean of the losses or negative log of probabilities, it is with this model the we achieved the best validation accuracy. We ran our model with up to 5 epochs as the validation accuracy reached plateau (on dinh) and remained (van) stagnant (tri tre) for every increase in epochs. The following code is how we constructed our CNN model in Python.
TESTING OUR MODEL IN REAL TIME
To test the most applicable use ( cach su dung tot nhat) of our model, we used the VideoCapture function in the CV2 library. The Cascade Classifier, designed by OpenCV, was used to detect the frontal (tran tren mat) face in the live video through “detectMultiScale”. A “while” loop was used to keep capturing images from the mirrored live video. The model will then determine (xac dinh) whether or not a face mask is worn in real time. Based on our model performance (hieu suat) and accuracy, the binary classifier result will be indicated (chi ra) by showing a green rectangle overlaid around the facial section signifying that the person on camera is wearing a mask, or a rectangle (hinh chu nhat) denoting (bieu thi) that the person on camera is not wearing a mask.

## RESULTS
With our chosen (da chon) model, we found the validation accuracy resulted at up to 61%. As true for all cases, there was some variance in the validation accuracy for each run of the code; This is primarily (chu yeu) due to the dropout layer affecting the training of the model somewhat differently each time the code is run from beginning, considering that neurons are selected at random by the layer.
While a validation accuracy of up to 61% is certainly ( chac chan) above random chance (co hoi, dip may), the result does not translate perfectly when tested live with OpenCV. There was an adequate (day du, can xung) number of times that model generated false positives and false negatives. Upon (tren) examination of the results, our model in practice seemed to be more sensitive to facial expressions and orientations, and also head positioning. We believe this is likely a byproduct of our relatively small but highly diverse dataset. As a result, our model is trained on a small number or images and finds difficulty in finding universal (pho cap) patterns (maaux) throughout (khap, khap moi noi) the datasets.
We also did testing on other models with alterations (su thay doi) to the composition (thanh phan) and number of layers. Other notable models that we trained were a model with an extra convolutional and pooling layer, a model with activation functions in dense layers, and a model with fewer dense layers. With the models that had an extra (them) convolution and pooling layer or activation functions in the dense layers, we did not notice a significant (co y nghia), consistent  difference to validation accuracy to make including the layers worth the additional computational  (tinh toan) costs and complexity (phuc tap) of the model. In the model with one fewer dense layer, we did notice a significant drop in validation accuracy quite consistently with the highest validation accuracy we found being roughly (khoang) 54%.

'Most of the job of deep learning consists (bao gom) of munging (trộn) data with Python scripts and then tuning (dieu chinh) the architecture (kien truc) and hyperparameters of a deep network at length to get a working model' — François Chollet, inventor of Keras
